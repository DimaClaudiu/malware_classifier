import os
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential, load_model
from keras.layers import Dense
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

FEATURES_PATH = "\X_features.txt"
LABELS_PATH = "\labels.txt"


def load_labels():
    labels = []
    with open(LABELS_PATH, 'r') as f:
        lines = f.readlines()
        labels = [int(line) for line in lines]

    return labels


def load_features():
    features = []
    vec = []
    append = False
    with open(FEATURES_PATH, 'r') as f:
        lines = f.readlines()
        for line in lines:
            if '[' in line:
                i = line.index('[')
                line = line[i+1:]

            if ']' in line:
                i = line.index(']')
                line = line[:i]

                append = True

            line = line.split()
            nums = [float(elem) for elem in line]

            vec = vec + nums

            if append == True > 0:
                features.append(vec)
                append = False
                vec = []

    return features


# Objects we'll need for training and testing
sc = StandardScaler()
classifier = Sequential()
LOAD_MODEL = True


# Data preparation
def prepare_data(features, labels, test_size=0.15):  # test_size: 23426 / 19912 ~= 0.85
    # Use np arrays for features/labels
    features = np.array(features).reshape(-1, 200)
    labels = np.array(labels)

    # No categorical features to label
    # No dummy variable traps to avoid

    # Split dataset into training and testing
    train_feats, test_feats, train_lbls, test_lbls = train_test_split(
        features, labels, test_size=test_size, random_state=0)

    # Perform feature scaling
    train_feats = sc.fit_transform(train_feats)
    test_feats = sc.transform(test_feats)

    return ((train_feats, train_lbls), (test_feats, test_lbls))


# Building and trining the Neural Network
def build_model(train_feats, train_lbls, save_model=True):
    # units = (nr independent vars + nr dependent vars) / 2 => 201 / 2
    classifier.add(
        Dense(units=100, kernel_initializer='uniform', activation='relu', input_dim=200))

    # Adding second layer, using relu for efficiency and reduced risk of gradient vanishing
    classifier.add(
        Dense(units=100, kernel_initializer='uniform', activation='relu'))

    # The output layer will use sigmoid, it seems to perform best on this dataset
    classifier.add(
        Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))

    # Using accuracy as metric since this is a malware classifier, we can't compromise on security
    classifier.compile(
        optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    classifier.fit(train_feats, train_lbls, batch_size=10, epochs=10)

    if save_model:
        classifier.save("model.h5")


# Assessing our model
def test_model(test_feats, test_lbls):
    pred_lbls = classifier.predict(test_feats)

    pred_lbls = (pred_lbls > 0.5)  # T/F instead of (0.0 - 1.0)
    print('Accuracy:', accuracy_score(test_lbls, pred_lbls))
    # True Positive Rates Against the False Positive Rate
    print('Area under ROC score: ', roc_auc_score(test_lbls, pred_lbls))

    # The confusion matrix gives us a good insigh on the models performance
    cm = confusion_matrix(test_lbls, pred_lbls)
    FN = cm[1][0]
    print(cm)
    plot_cm(cm)

    # False negatives are really dangerous for security, we need to eliminate those
    # False positives aren't as bad, but still undesireable
    print("False Negatives:", FN, " -",  FN/len(test_lbls),  '%')


def plot_cm(cm):
    classes = ['Negative', 'Positive']
    marks = np.arange(len(classes))
    tf = [['TN', 'FP'], ['FN', 'TP']]

    plt.clf()
    plt.title('Malware Classification - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.xticks(marks, classes, rotation=45)
    plt.yticks(marks, classes)
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)

    for i in range(2):
        for j in range(2):
            plt.text(j, i, str(tf[i][j])+" = "+str(cm[i][j]))
    plt.show()


# Classifies malware based on a sample(list) of size 200
def predict_input(features):
    features = np.array(features).reshape(-1, 200)
    transformed_feats = sc.fit_transform(features)

    pred_lbls = classifier.predict(transformed_feats)
    pred_lbls = (pred_lbls > 0.5)

    return pred_lbls


# Entry point
if __name__ == '__main__':
    # Load data
    features = load_features()
    labels = load_labels()

    train_test = prepare_data(features, labels)
    train = train_test[0]
    test = train_test[1]

    # Also trained a model locally that has slightly improved perforce
    if LOAD_MODEL:
        classifier = load_model(
            "local_model.h5")
    else:
        build_model(*train)

    test_model(*test)
    # predict_input(input())
